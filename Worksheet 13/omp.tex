% Author: Rishi Veerapeneni, Jonathan Shi
\\ \\
\learning{
    Understand the motivation for OMP and how the algorithm works.
}

OMP is an algorithm that solves an equation of the form $A\vec{x} = \vec{b}$.
This may look familiar, as we have discussed other ways of solving this equation,
like Gaussian elimination, inverse matrices, or least squares.
In this question, we will explore when and why we would want to use OMP instead of
these other techniques.
\\ \\
\note {
    This first half of the problem focuses on the motivation of and assumptions made in the OMP algorithm.
}

\begin{enumerate}
    \item In the equation $A\vec{x} = \vec{b}$, what are $A$, $\vec{x}$, and $\vec{b}$?
    Specifically, what are their dimensions, and what do they mean in the context of
    a real-world problem like GPS trilateration? 
    \begin{itemize}
        \item What does $A$ represent? What are its dimensions? What does the $i$th column of $A$ represent?
        \answerbox{0.5cm}
        \\ \\
        \sol{
            $A$ is an $n\times m$ matrix, where $m$ is the number of devices and $n$ is the signal
            broadcasted by each device. Therefore, the $i$th column of $A$ is the signal from the $i$th device.
        }

        \item What does $\vec{x}$ represent? What are its dimensions? What does $x_i$ represent?
        \answerbox{0.5cm}
        \\ \\
        \sol{
            $\vec{x}$ is an $m\times 1$ matrix, so we just say it is a vector of size $m$.
            $x_i$ represents the scaling factor (and the strength) of the signal sent out by the $i$th device.
        }

        \item What does $\vec{b}$ represent? What are its dimensions?
        \answerbox{0.5cm}
        \\ \\
        \sol{
            $\vec{b}$ is an $n\times 1$ matrix, so we just say it is a vector of size $n$.
            $\vec{b}$ represents the signal received by the receiver.
        }
    \end{itemize}

    \item To solve $A\vec{x}=\vec{b}$, what assumptions do we need to make in order to use each of the following techniques?
    \\ \\
    \note{
        Try to explain how each of these assumptions would be violated in the context of the GPS problem.
    }
    \begin{itemize}
        \item Gaussian elimination
        \answerbox{1cm}
        \\ \\
        \sol{There must be no noise in our received signal $\vec{b}$.}
        \item Inverse matrices
        \answerbox{1cm}
        \\ \\
        \sol{
            $A$ must be square and its columns must be linearly independent.
            Note that GE can be used even when $A$ is not square.
        }
        \item Least squares
        \answerbox{1cm}
        \\ \\
        \sol{
            $A^TA$ must be invertible, which in turn means $A$ must have a trivial nullspace
            (see \href{http://inst.eecs.berkeley.edu/~ee16a/fa19/lecture/Note23.pdf}{Note 23} for more details.)
            This also means $A$ must have linearly independent columns.
        }
    \end{itemize}

    \item What specific assumptions does the OMP algorithm make? \textit{
        Hint: there's three main ones.
    }
    \answerbox{2cm}
    \\ \\
    \sol{
        \begin{itemize}
            \item There are many devices that could be broadcasting signals
            \item The signals sent by each device are nearly orthogonal to each other
            \item Only a few devices are actually sending out signals, i.e.
                  there are few $i$ for which $x_i$ would be nonzero.
        \end{itemize}
    }
    \item What do these assumptions imply about
    \begin{itemize}
        \item the dimensions of $A$, the matrix whose columns are all our possible beacons?
        \answerbox{0.5cm}
        \item the relationship between $\vec{s}_i$ and $\vec{s}_j$, two of our possible beacon signals?
        \answerbox{0.5cm}
        \item the scaling factors in $\vec{x}$?
        \answerbox{0.5cm}
    \end{itemize}
    \sol{
        \begin{itemize}
            \item $A$ should be short and wide, as we have many possible signals
            \item $<\vec{s}_i, \vec{s}_j> \approx 0$, as our signals are nearly orthogonal
            \item most $x_i$ are 0, as $\vec{x}$ is sparse
        \end{itemize}
        Note that $A$ here is the $A$ in our original equation $A\vec{x}=\vec{b}$,
        not the same as the $A$ constructed by the OMP algorithm. The $A$ constructed by OMP
        simply omits all beacons that we believe to not be broadcasting, i.e. $\vec{s}_i$ for which $x_i$ is 0.
    }

    OMP comes with one more twist: previously, we received a linear combination of our beacons signals, where
    $$ \vec{r} = \alpha_1\vec{s}_1 + \dots + \alpha_m\vec{s}_m $$
    where $\vec{r}$ is the received signal, and each $\vec{s}_i$ is a beacon signal scaled by a factor of $\alpha_i$.
    \\ \\
    In OMP, instead of just receiving a linear combination of beacons, we receive a linear
    combination of \textit{shifted versions} of our beacon signals. In other words,
    $$ \vec{r} = \alpha_1\vec{s}_1^{(\tau_1)} + \dots + \alpha_m\vec{s}_m^{(\tau_m)} $$
    where signal $\vec{s}_i^{(\tau_i)}$ denotes $\vec{s}_i$ shifted forward in time by $\tau_i$.
    \item How does receiving shifted versions of signals affect our assumptions about their orthogonality?
    \\ \\
    \note{
        It may help to draw on the board what it means to shift the signal \textit{forward} in time,
        just to remind your students.
    }
    \answerbox{0.5cm}
    \\ \\
    \sol{
        Instead of our signals being nearly orthogonal to each other, they must now be nearly orthogonal
        to all shifted versions of themselves, i.e.
        $$ <\vec{s}_i^{(\tau_i)}, \vec{s}_j^{(\tau_j)}> \approx 0 $$
        where $i \neq j$ and $\tau_i \neq \tau_j$.
    }
    \\ \\
    We now know what assumptions are necessary to apply OMP. Now let's talk about the algorithm!
    OMP as a whole revolves around a simple strategy. We keep track of how much of the received signal is
    still unaccounted for, find the strongest device within that signal, subtract out that component from
    the unaccounted for signal, and repeat until we can't find more beacon signals.
    At a high level, this is what the steps of the algorithm look like:
    \\ \\
    \note{
        We now focus on the steps of the algorithm itself.
    }
    \begin{itemize}
        \item We initialize $A_{curr}$ to be our matrix of beacons we know to be a part of the received signal $\vec{r}$,
        and $\vec{x}_{curr}$ to be our guesses for the weights of these beacons within $\vec{r}$.
        \item We assume there to be at most $k$ beacons that are broadcasting.
        \item While the magnitude of the difference between our guess for $\vec{r}$ and the actual value of $\vec{r}$ is
        above some threshhold AND we've not yet found $k$ beacons:
        \begin{itemize}
            \item Find the strongest beacon yet unaccounted for, and add it to $A_{curr}$.
            \item Update your estimate for $\vec{x}_{curr}$.
        \end{itemize}
    \end{itemize}

    \item How can we encode each of these steps in mathematical terms?
    \begin{enumerate}
        \item Let $\vec{e}$ be the difference between our guess of $\vec{r}$ and the actual value of $\vec{r}$.
        How do we write an expression for $\vec{e}$.
        \\ \\
        \sol{
            $\vec{e}$ represents the amount of the original signal that's yet unaccounted for by our guess.
            Our guess is $A_{curr}\vec{x}_{curr}$, so our error vector is
            $$\vec{e} = \vec{r} - A_{curr}\vec{x}_{curr}$$
        }
        \item Given $A_{curr}$ and $\vec{x}_{curr}$, how do we find the strongest remaining shifted signal $\vec{s}_i^{(\tau_i)}$?
        \textit{Hint: How do we get an expression for how much of the signal is still unaccounted for by the vectors in $A$?}
        \\ \\
        \sol{
            $\vec{e}$, found in the previous problem, actually describes how much of the signal is not yet accounted for.
            To find the strongest beacon, we now just cross-correlate $\vec{e}$ with each $\vec{s}_i$. The correlation with
            the largest value tells us which $\vec{s}_i$ it is, and the index within the correlation vector is the
            amount this signal was shifted by.
        }
        \item Given guessed signals $A_{curr}$, received signal $\vec{r}$, the signal we just found $\vec{a}$, and old estimate
        $\vec{x}_{curr, old}$, how do we get our new estimate for $\vec{x}_{curr, new}$?
        \\ \\
        \sol{
            We use least squares to approximate $\vec{x}$.
            $$ \vec{x}_{curr, new} = (A^TA)^{-1}A^T\vec{r} $$
        }
    \end{enumerate}
    Let's walk through the algorithm with some example values now.
    % The below numbers are a subset of those provided in Note 24
    % http://inst.eecs.berkeley.edu/~ee16a/fa19/lecture/Note24.pdf
    Suppose we have the following:
    \begin{itemize}
        \item $k=2$ maximum number of signals
        \item beacon signals $\vec{s}_1 = \begin{bmatrix} -0.5 \\ 0.5 \\ 0.1 \end{bmatrix}$,
            $\vec{s}_2 = \begin{bmatrix} 0.8 \\ 0.6 \\ 0.7 \end{bmatrix}$,
            and $\vec{s}_3 = \begin{bmatrix} 0 \\ -0.6 \\ 0 \end{bmatrix}$
        \item received signal $\vec{r} = \begin{bmatrix} -1.8 \\ 1.5 \\ 0.3\end{bmatrix}$
    \end{itemize}
    \item At the start of our algorithm, how much of the received signal is still unaccounted for?
    \\ \\
    \sol{
        Initially, we've found 0 beacon vectors, so the entire received signal $\vec{r}$ is still
        unaccounted for.
    }
    \item Next, find the strongest unaccounted for beacon within $\vec{e}$.
    \\ \\
    \sol{
        Per the previous part, we initialize $\vec{e}=\vec{r}$.
        We examine the dot products of $\vec{e}$ with all shifted versions of our signal vectors.
        \begin{itemize}
            \item $<\vec{e}, \vec{s}_1^{(0)}>$ = 1.68
            \item $<\vec{e}, \vec{s}_1^{(1)}>$ = -0.9
            \item $<\vec{e}, \vec{s}_1^{(2)}>$ = -0.78
            \item $<\vec{e}, \vec{s}_2^{(0)}>$ = -0.33
            \item $<\vec{e}, \vec{s}_2^{(1)}>$ = 0.21
            \item $<\vec{e}, \vec{s}_2^{(2)}>$ = 0.12
            \item $<\vec{e}, \vec{s}_3^{(0)}>$ = -0.9
            \item $<\vec{e}, \vec{s}_3^{(1)}>$ = 1.08
            \item $<\vec{e}, \vec{s}_3^{(2)}>$ = -0.18
        \end{itemize}
        The largest magnitude is in the inner product between $\vec{e}$ and $\vec{s}_1^{(0)}$.
        This tells us that we $\vec{s}_1^{(0)}$ is the strongest beacon.
    }
    \item Estimate our new value of $\vec{x}_{curr}$.
    \\ \\
    \sol{
        We perform least squares with $A_{curr} = \begin{bmatrix} \vline \\ \vec{s}_1^{(0)} \\ \vline \end{bmatrix}$ to give us
        $$
            \vec{x}_{curr} = (\begin{bmatrix} -0.5 & 0.5 & 0.1 \end{bmatrix}
            \begin{bmatrix} -0.5 \\ 0.5 \\ 0.1 \end{bmatrix})^{-1}
            \begin{bmatrix} -0.5 & 0.5 & 0.1 \end{bmatrix}
            \begin{bmatrix} -1.8 \\ 1.5 \\ 0.3 \end{bmatrix}
            \approx \begin{bmatrix} 3.294 \end{bmatrix}
        $$
        We're slightly off the mark (since the actual value is 3), but this is pretty close, and will improve
        as we improve our estimate.
    }
    \item After the last step, how much of $\vec{r}$ remains unaccounted for?
    \\ \\
    \sol{
        \begin{align*}
            \vec{e} &= \vec{r} - A_{curr}\vec{x}_{curr} \\
            &= \begin{bmatrix} -1.8 \\ 1.5 \\ 0.3\end{bmatrix}
                - \begin{bmatrix} -0.5 \\ 0.5 \\ 0.1 \end{bmatrix}
                \begin{bmatrix} 3.294 \end{bmatrix} \\
            &\approx \begin{bmatrix} -0.152 \\ -0.147 \\ -0.029 \end{bmatrix}
        \end{align*}
    }
    \item Find the next strongest unaccounted beacon.
    \\ \\
    \sol{
        We cross-correlate with the remaining vectors as follows.
        \begin{itemize}
            \item $<\vec{e}, \vec{s}_2^{(0)}>$ = -0.231
            \item $<\vec{e}, \vec{s}_2^{(1)}>$ = -0.218
            \item $<\vec{e}, \vec{s}_2^{(2)}>$ = -0.242
            \item $<\vec{e}, \vec{s}_3^{(0)}>$ = 0.088
            \item $<\vec{e}, \vec{s}_3^{(1)}>$ = 0.092
            \item $<\vec{e}, \vec{s}_3^{(2)}>$ = 0.018
        \end{itemize}
        The inner product of $\vec{e}$ with $\vec{s}_2^{(2)}$ has the largest magnitude.
    }
    \\ \\
    \note{
        We take the inner product with the largest \textit{magnitude} rather than the largest value to
        account for the possibility of negative scaling.
    }
    \item Once more, estimate our new value of $\vec{x}_{curr}$.
    \\ \\
    \sol{
    We do least squares again, this time with $A = \begin{bmatrix} \vline & \vline \\ \vec{s}_1^{(0)} & \vec{s}_3^{(2)} \\ \vline & \vline \end{bmatrix}$.
        $$
            \vec{x}_{curr} = (\begin{bmatrix} -0.5 & 0.5 & 0.1 \\ 0.7 & 0.8 & 0.6 \end{bmatrix}
            \begin{bmatrix} -0.5 & 0.7 \\ 0.5 & 0.8 \\ 0.1 & 0.6 \end{bmatrix})^{-1}
            \begin{bmatrix} -0.5 & 0.5 & 0.1 \\ 0.7 & 0.8 & 0.6 \end{bmatrix}
            \begin{bmatrix} -1.8 \\ 1.5 \\ 0.3 \end{bmatrix}
            = \begin{bmatrix} 3.178 \\ 0.102 \end{bmatrix}
        $$
    }
    \\ \\
    We can now terminate OMP, as we have performed 2 iterations of the algorithm, thereby finding $k=2$ signals.
    \item Express $\vec{r}$ as a linear combination of shifted versions of $\vec{s}_1$, $\vec{s}_2$, and $\vec{s}_3$.
    What is our final error vector $\vec{e}$?
    \\ \\
    \sol{
        The result of OMP tells us that $\vec{r} = 3.178\vec{s}_1^{(0)} + 0.102\vec{s}_2^{(2)}$.
        Our error vector $\vec{r} - A_{curr}\vec{x}_{curr}$ turns out to be
        $\begin{bmatrix}-0.282 \\ -0.170 \\ 0.556\end{bmatrix}$.
    }
    \\ \\
    \note{
        If your students have some difficulty understanding this part, remind them of the fact that
        a matrix-vector multiplication is a linear combination of the columns of the matrix, i.e.
        $$
            \begin{bmatrix} \vline & & \vline \\ \vec{a}_1 & \dots & \vec{a}_n \\ \vline & \vline \end{bmatrix}
            \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
            = x_1\vec{a}_1 + \dots x_n\vec{a}_n
        $$
    }
\end{enumerate}